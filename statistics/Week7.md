# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `2부-데이터 분석 준비하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다


## Statistics_7th_TIL

### 2부. 데이터 분석 준비하기

### 11. 데이터 전처리와 파생변수 생성

<!-- 11.5 모델 성능 향상을 위한 파 변수 생성부터 11장 끝까지 진행해주시면 됩니다.-->

## Study ScheduleStudy Schedule

| 주차  | 공부 범위     | 완료 여부 |
| ----- | ------------- | --------- |
| 1주차 | 1부 p.2~46    | ✅         |
| 2주차 | 1부 p.47~81   | ✅         |
| 3주차 | 2부 p.82~120  | ✅         |
| 4주차 | 2부 p.121~167 | ✅         |
| 5주차 | 2부 p.168~202 | ✅         |
| 6주차 | 2부 p.203~250 | ✅         |
| 7주차 | 2부 p.251~299 | ✅         |

<!-- 여기까진 그대로 둬 주세요-->



---

# 1️⃣ 개념 정리 

## 11.데이터 전처리와 파생변수 생성

```
✅ 학습 목표 :
* 결측값과 이상치를 식별하고 적절한 방법으로 처리할 수 있다.
* 데이터 변환과 가공 기법을 학습하고 활용할 수 있다.
* 모델 성능 향상을 위한 파생 변수를 생성하고 활용할 수 있다.
```

### 11.5. 모델 성능 향상을 위한 파생 변수 생성

1. 파생변수: 원래 있던 변수들을 조합하거나 함수를 적용하여 새로 만들어낸 변수
   - 예) X1과 X2 두 변수의 평균값으로 만든 X3, 데이터 구간화, 표준화 및 정규화 등
2. 전체 데이터에 대한 파악이 중요할 뿐만 아니라 해당 비즈니스 도메인에 대한 충분한 이해가 수반되어야 함
3. 데이터의 특성과 흐름을 파악한 후 아이디어를 얻어서 만드는 것이 효과적
4. 파생변수 예시 목록
<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/4907f3d2-1ff4-4239-b47d-e81b7c775e15" />

5. 다중공선성 문제가 발생할 가능성이 높음
   - 파생변수를 만든 후 상관분석을통해 변수 간의 상관성을 확인해야 함 


### 11.6. 슬라이딩 윈도우 데이터 가공

1. 실시간 네트워크 패킷 데이터를 처리하는 기법
2. 현재 시점으로부터 ----- | ------------- | --------- |
| 1주차 | 1부 p.2~46    | ✅         |
| 2주차 | 1부 p.47~81   | ✅         |
| 3주차 | 2부 p.82~120  | ✅         |
| 4주차 | 2부 p.121~167 | ✅         |
| 5주차 | 2부 p.168~202 | ✅         |
| 6주차 | 2부 p.203~250 | ✅         |
| 7주차 | 2부 p.251~299 | ✅         |

<!-- 여기까진 그대로 둬 주세요-->



---

# 1️⃣ 개념 정리 

## 11.데이터 전처리와 파생변수 생성

```
✅ 학습 목표 :
* 결측값과 이상치를 식별하고 적절한 방법으로 처리할 수 있다.
* 데이터 변환과 가공 기법을 학습하고 활용할 수 있다.
* 모델 성능 향상을 위한 파생 변수를 생성하고 활용할 수 있다.
```

### 11.5. 모델 성능 향상을 위한 파생 변수 생성

1. 파생변수: 원래 있던 변수들을 조합하거나 함수를 적용하여 새로 만들어낸 변수
   - 예) X1과 X2 두 변수의 평균값으로 만든 X3, 데이터 구간화, 표준화 및 정규화 등
2. 전체 데이터에 대한 파악이 중요할 뿐만 아니라 해당 비즈니스 도메인에 대한 충분한 이해가 수반되어야 함
3. 데이터의 특성과 흐름을 파악한 후 아이디어를 얻어서 만드는 것이 효과적
4. 파생변수 예시 목록
<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/4907f3d2-1ff4-4239-b47d-e81b7c775e15" />

5. 다중공선성 문제가 발생할 가능성이 높음
   - 파생변수를 만든 후 상관분석을통해 변수 간의 상관성을 확인해야 함 


### 11.6. 슬라이딩 윈도우 데이터 가공

<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/d4a11706-4fc4-47da-ac5d-fe5f5cb4fd4f" />


1. 실시간 네트워크 패킷 데이터를 처리하는 기법
2. 현재 시점으로부터 ±M 기간의 데이터를 일정 간격의 시간마다 전송하는 방식
3. 각각의 데이터 조각(window)들이 서로 겹치며 데이터가 전송됨
4. 데이터가 겹치도록 쪼개어 전송하는 이유
   - 패킷 전송 후 전송을 확인받지 않고도 곧바로 다음 패킷을 보낼 수 있어 네트워크를 효율적으로 사용할 수 있기 때문
5. 데이터를 겹쳐 나눔으로써 전체 데이터가 증가하는 원리를 차용한 것이 슬라이딩 윈도우 데이터 가공의 핵심
6. 예측 모델에서 유용하게 쓰임
7. 데이터가 충분하지 않은 경우, 많은 분석 데이터셋을 확보하고 학습 데이터의 최근성을 가질 수 있음
8. 슬라이딩 윈도우의 테이블 구조
<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/99a6df20-e9bd-4a78-9fac-3aed5ed72282" />


### 11.7. 범주형 변수의 가변수 처리

<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/4f93dc64-3f88-47a6-8f2c-bc8f8cb7d545" />
<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/9944a64d-791c-4243-af0f-a193fbbe55cf" />

1. 가변수
   - 더미 변수, 이진변수, 불리언변수라고도 불림
   - 범주형 변수를  0과 1의 값을 가지는 변수로 변환하는 것을 의미
2. 가변수를 만드는 이유
   - 범주형 변수는 사용할 수 없고 연속형 변수만 사용 가능한 분석기법을 사용하기 위함
3. 범주의 개수보다 하나 적게 가변수를 만들어야 함
   - 이유: 마지막 범주는 나머지 변수가 모두 '0'이면 해당 범주가 '1'인 것을 알 수 있기 때문
   - 일반적으로 종속변수에 대한 영향력이 가장 적은 범주를 제거
   - 제거된 범주를 baseline이라 함
   - baseline 범주의 종속변수에 대한 영향력은 0으로 맞춰짐
   - 각각의 변수는 독립성을 가지고 있어야 함
     - 다중공선성 문제를 예방하기 위해
       

### 11.8. 클래스 불균형 문제 해결을 위한 언더샘플링과 오버샘플링

#### 클래스 불균형
1. 특히 0과 1의 이진분류 모델에서 1의 비율이 매우 적은 클래스 불균형 문제가 발생하는 경우가 많음
2. 클래스 불균형이 심하게 되면 예측 정확도가 떨어짐
3. 일반적인 기계학습 분류 모델은, 적은 비중의 클래스든 큰 비중의 클래스든 중요도에 차별을 두지 않고 전체적으로 분류를 잘 하도록 학습됨

#### 데이터 불균형 문제를 해결하는 방법

##### 가중치 밸런싱
- 모델 자체에 중요도가 높은 클래스에 정확도 가중치를 주어 특정 클래스의 문류 정확도가 높아지도록 조정
- 중요도가 높은 클래스를 잘못 분류하면 더 큰 손실을 계산하도록 조정

##### 언더샘플링

<img width="200" height="300" alt="image" src="https://github.com/user-attachments/assets/15e41454-eadb-4869-8e4f-def5c220b19c" />

1. 큰 비중의 클래스 데이터를 줄이는 방법
2. 큰 비중의 클래스 데이터만큼만 추출하여 학습시킴
3. 언더샘플링 기법
   - 랜덤 언더샘플링
     - 작은 비중의 클래스와 관측치 비율이 유사해질 때까지 무작위로 큰 비중의 클래스 관측치를 제거
   - EasyEnsemble
     - 앙상블 기법
     - 소수 클래스는 그대로 두고 다수 클래스를 여러 번 무작위로 부분 샘플링하여 각각의 밸런스된 데이터셋으로 여러 분류기를 학습시킴
   - Condensed Nearest Neighbor(CNN)
     - K-근접이웃 모델을 차용한 언더샘플링 방법
     - 비중이 큰 클래스의 관측치 중 비중이 작은 클래스와 공간상 위치가 맞닿는 부분의 관측치만 남김
     - 작동방식
       1. 비중이 작은 클래스만 있는 집합 S에 비중이 큰 클래스의 관측치 하나를 포함시킴
       2. 포함시킨 비중이 큰 클래스의 관측치를 K-근접이웃 방식으로 분류(기본적으로 1NN)
       3. 만약 분류가 틀렸으면 크 표본을 집합 S에 포함시킴
       4. S set에 포함되어있지 않은 모든 값의 분류가 집합 S로 배정이 가능할 때까지 1~3회 반복
  
##### 오버샘플링

<img width="300" height="300" alt="image" src="https://github.com/user-attachments/assets/945632cc-d519-484e-af71-05f5af2937c9" />

1. 작은 비중의 클래스 데이터를 늘리는 방법
2. 비중이 작은 클래스의 관측치 수와 동일하도록 작은 비중의 클래스 관측치들을 증가시킴
3. 오버샘플링 기법
   - 랜덤 오버샘플링
     - 작은 클래스의 관측치를 단순히 무작위로 선택하여 반복 추출
   - Synthetic Minority Over-Sampling Technique(SMOTE)
     - 대표적인 오버샘플링 기법
     - K-근접이웃 기법을 사용
     - 비중이 작은 클래스의 관측치의 K 최근접 이웃 관측치들을 찾아서, 해당 관측발

### 11.9. 데이터 거리 측정 방법

<img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/fe78c77a-de68-4789-aed0-4ccc561a9995" />

- 데이터 거리 측정: 관측치 A를 기준으로, B와 C 중 어느 관측치가 더 가까이 있는가를 판단하기 위한 것
- 데이터 유사도 측정이라고도 함

#### 대표적인 거리 측정 방법

##### 유클리드 거리

<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/6774261f-4a14-4370-8594-bdf6d3899b9b" />

- 피타코라스 정리 활용
- 관측치 간의 직선 거리를 측정
- 개념이 매우 직관적이고 실제 거리를 사용
- 유클리드 거리 값이 0에 가까울수록 데이터 간의 거리가 짧음을 의미
- <img width="300" height="100" alt="image" src="https://github.com/user-attachments/assets/77ec41a3-9be4-4c91-a943-b1ce73fed1a6" />

##### 맨해튼 거리

<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/0ebbe8a2-eebf-4e54-bb90-4cea39ca7373" />

- 택시 거리라고도 불림
- 맨해튼의 격자 모양 도로에서 최단거리를 구하는 원리를 이용
- 맨해튼 거리는 L1 Norm이라 불리며, L2 Norm은 유클리드 거리임
- A지점과 B지점까지의 X축 거리와 Y축 거리를 합해주면 됨
- <img width="300" height="100" alt="image" src="https://github.com/user-attachments/assets/651eda93-98b5-4d3a-9d89-e6aaa5cc350b" />

##### 민코프스키 거리

- 옵션값을 설정하여 거리 기준을 조정할 수 있는 거리 측정 방법
- 유클리드 거리 수식과 동일하며, 단지 제곱 부분을 p-norm값으로 조정 가능
- p값을 1로 설정하면 맨해튼 거리와 동일하고, 2로 설정하면 유클리드 거리와 동일
- p값은 반드시 1 이상이어야 하고, 정수가 아니어도 됨
- <img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/9e050fc7-ee98-4b4a-b3b8-a1d8000d74df" />

##### 체비쇼프 거리

- 민코프스키 거리의 p값을 무한대로 설정했을 때의 거리
- 맥시멈 거리라고도 함
- 군집 간의 최대 거리를 구할 때 사용
- 계산값이 0에 가까울수록 유사함
- <img width="300" height="100" alt="image" src="https://github.com/user-attachments/assets/a7b62228-9a91-43f2-b73e-3aa7ef2036c0" />

##### 마할라노비스 거리

<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/9887ce71-843e-4cb0-a7a6-53e40a9ce2e9" />

- 유클리드 거리에 공분산을 고려한 거리 측정 방법
  - X와 Y의 공분산을 고려하여 거리를 측정
- 변수 내 분산과 변수 간 공분산을 모두 반영하여 A와 B간 거리를 계산
- 단순 거리의 상관성을 함께 볼 수 있음
- 확률 분포를 고려하기 때문에 공분산 행렬을 사용
- <img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/6378ffe7-8865-4b0f-83ff-178c62a6f989" />

##### 코사인 거리

<img width="500" height="300" alt="image" src="https://github.com/user-attachments/assets/f8b992cc-d330-45c8-8ca6-8c8773a96eee" />

- 코사인 유사도: 두 벡터의 사이각을 구해서 유사도를 구하는 것
  - -1에서 1 사이의 값을 가짐
  - 두 벡터의 방향이 완전히 동일하면 1의 값을 가짐
  - 일반적으로 코사인 유사도는 양수 변수를 사용하기 때문에 0에서 1 사이의 값을 가짐
  - <img width="300" height="100" alt="image" src="https://github.com/user-attachments/assets/e2ffecc9-30e7-4cc3-8de6-6b24180d9164" />

  - 코사인 유사도는 벡터의 각도만으로 유사도를 판단해도 무방할 때 사용
  - 협업 필터링 모델이나 문서 간 유사도를 측정하는 모델에서 좋은 성능을 보임
- 1에서 코사인 유사도를 빼면 그게 바로 코사인 거리
- 코사인 유사도가 높을수록 거리는 줄어들음





<br>
<br>

---

# 2️⃣ 확인 과제

> **교재에 있는 실습 파트를 직접 따라 해보세요. 실습을 완료한 뒤, 결과화면(캡처 또는 코드 결과)을 첨부하여 인증해 주세요.**
>
> **단순 이론 암기보다, 직접 손으로 따라해보면서 실습해 보는 것이 가장 확실한 학습 방법입니다.**
>
> > **인증 예시 : 통계 프로그램 결과, 시각화 이미지 캡처 등**

<!-- 이 주석을 지우고 “실습 결과 화면(캡처)을 이곳에 첨부해주세요.-->

~~~
인증 이미지가 없으면 과제 수행으로 인정되지 않습니다.
~~~



---

# 3️⃣ 실습 과제 (마지막 과제)

>  **🧚Q. 마지막 과제는 다음과 같습니다. 『데이터 분석가가 반드시 알아야 할 모든 것』 2부를 마무리하는 주차로,그동안 배운 데이터 전처리 및 파생변수 생성 내용을 실제 데이터에 적용해 보는 실습형 과제입니다. 단순히 함수를 실행하는 데서 그치지 않고, "왜 이 전처리 방법을 선택했는가" 와 "데이터가 말해주는 인사이트는 무엇인가'를 중심으로 EDA(탐색적 데이터 분석)를 함께 수행해주세요.**
>
> (정규과제 업로드 시트에 과제를 수행한 Git 링크와 코랩도 같이 올려주세요) 

<!-- 4주차 과제부터 실습하면서 배운 파이썬 문법을 적용하면서 실습을 진행해주세요 -->

~~~
과제 가이드라인

1. 실습 데이터셋 불러오기
Kaggle : Students Performance in Exams
- 출처: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams
- 설명:
미국 고등학생 1000명의 성적과 배경 요인(성별, 인종, 부모 학력, 점심 여부, 시험 준비 과정 등)을 담은 데이터입니다.
math score, reading score, writing score 3가지 점수를 기준으로
학업 성취에 영향을 미치는 요인을 분석해볼 수 있습니다.

2. 데이터 전처리 진행하기
교재에서 배웠던 개념들을 적용해면서 전처리를 진행해봅시다. 
- 결측값 처리, 이상치 처리, 스케일링 등 
- (Optional) 범주형 변수 인코딩, 파생 변수 생성

3. EDA (탐색적 데이터 분석)
전처리된 데이터를 바탕으로 자유롭게 시각화 및 요약 분석을 수행하세요. 
- 점수 간 상관관계 분석
- 그룹 별 비교
- 여러 과정에 따른 성적 분포 비교
- 변수 간 관계 시각화 

4. 주석이나 코드 설명에서 들어가야 할 부분
- 교재에 있는 어떤 통계 개념을 적용했는지
- 각 개념이 데이터 분석에서 어떻게 활용되었는지를 스스로 설명해보세요.
- 단순한 코드 작성보다, 통계 개념 -> 코드 적용 -> 해석 -> 배운 점의 흐름을 명확히 드러내는 것이 핵심 기준입니다. 
~~~



<!-- 이것으로 통계학 정규과제가 마무리 되었습니다.  자료실에서 보면 아시겠지만, 이번 통계학 정규과제는 2부까지만 진행을 하였습니다. 3부부터는 모델에 대한 개념이 등장하기 때문에, 수학적 통계학을 배우고 분석의 기초를 다지는 부분에 여러분이 더 집중할 수 있도록 구성했습니다. 또한 전체 분량이 길기 때문에 학습 부담을 줄이기 위한 결정입니다. 따라서 이번 주차를 끝으로 정규 과제는 마무리되지만, 머신러닝 모델에 대해 더 깊이 공부하고 싶은 분들은 3부를 개인적으로 학습해보는 것을 추천드립니다. 그동안 과제를 열심히 하느라 고생하셨습니다. -->

### 🎉 수고하셨습니다.
